lr: 0.0001
gamma: 0.99
lmbda: 0.95
clip_ratio: 0.2
value_coef: 1.0
entropy_coef: 0.0001
total_time_steps: 100_000_000
num_steps: 32
num_mini_batches: 32
num_envs: 2048
num_epochs: 5
max_grad_norm: 1.0
normalize_advantages: True
normalize_env: False
anneal_lr: False
num_eval: 100
max_episode_steps: 1000
hidden_dim: 512
use_tanh_gaussian: False
loss: ppo



network:
  _target_: src.algorithms.ppo.networks.PPONetworks
  _partial_: true
init:
  _target_: src.algorithms.ppo.ff_ppo.make_init_fn
  _partial_: true
learner:
  _target_: src.algorithms.ppo.ff_ppo.make_learner_fn
  _partial_: true
policy:
  _target_: src.algorithms.ppo.ff_ppo.make_policy_fn
  _partial_: true
